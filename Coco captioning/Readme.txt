In the context of the project, the Coco dataset, a large collection of annotated images,
was used to address the captioning problem. Captioning involves providing a meaningful textual description for each image in the dataset. 
To achieve this goal, a combination of advanced deep learning techniques was adopted.
Specifically, a Convolutional Neural Network (CNN) was utilized to automatically extract salient features from the images. 
The CNN demonstrated a high capacity for learning visual structures,
 enabling the encoding of image signals into a format understandable by the captioning system.
Subsequently, for text generation, a Recurrent Neural Network (RNN) was employed.
 RNNs are well-suited for handling sequential data and were used to decode the features extracted by the CNN
 into a coherent and meaningful sequence of words, thus composing the complete image description.
The approach of using a combination of CNN and RNN proved effective in tackling the captioning challenge, 
allowing the system to provide image descriptions with good semantic coherence and an understanding
 of the visual relationships present in the photographed scenes.